{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ab2f35",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Understanding ConvLSTM\"\n",
    "author: \"Kirtan Gangani\"\n",
    "date: \"July 13, 2025\" \n",
    "categories: [CNN, LSTM]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: false\n",
    "    code-copy: true\n",
    "jupyter: python3\n",
    "image: \"convlstm.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2eb15",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the world of deep learning, we've witnessed incredible breakthroughs. **Convolutional Neural Networks (CNNs)** have revolutionized the way we interpret **images and spatial data**, while **Recurrent Neural Networks (RNNs)**, particularly **Long Short-Term Memory (LSTM) networks**, have become indispensable for understanding **sequential information** like text and time series.\n",
    "\n",
    "But what happens when your data is both spatial and sequential? Think about video: it's a sequence of images. How do you analyze dynamic patterns that evolve in both space and time? This is where traditional networks often struggle, and it's precisely the challenge that ConvLSTM was designed to address.\n",
    "\n",
    "In this blog post, we will understand the fundamental building blocks, understand their individual limitations, and then the solution that ConvLSTM offers for understanding dynamic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e91ed",
   "metadata": {},
   "source": [
    "# The Building Blocks: CNNs and LSTMs\n",
    "\n",
    "Let's first quickly revisit its two powerful parents.\n",
    "\n",
    "## CNNs\n",
    "\n",
    "CNNs are a specialized type of neural network designed for **processing grid-like data**. This makes them exceptionally good at handling images, video frames, and other spatial datasets. Their power comes from their ability to automatically **learn** hierarchical spatial features **directly from raw input** (e.g., pixels). This is done in two steps:  \n",
    "1. Early layers detect **specific local patterns** like edges, corners and textures.  \n",
    "2. Deeper layers then combine these to **recognize complex objects and patterns**.\n",
    "\n",
    "![](cnn.png)\n",
    "\n",
    ": Figure 1: A conceptual diagram of a Convolutional Neural Network, showing how spatial features are extracted and downsampled through convolutional and pooling layers.\n",
    "\n",
    "As you can see in Figure 1, CNNs apply **filters** (kernels) across the input to create **feature maps**, progressively reducing the spatial dimensions while increasing feature complexity. This architecture is inherently designed to understand where features are located in space.\n",
    "\n",
    "## LSTMs\n",
    "\n",
    "LSTMs are a special type of Recurrent Neural Network (RNN) specifically designed to **process sequential data**. They were introduced to **mitigate** common problems in standard RNNs, such as the **vanishing or exploding gradient problem**, which made it difficult for RNNs to learn long-term dependencies.\n",
    "\n",
    "The core of an LSTM's power lies in its Cell State (Memory) and its \"gates\":\n",
    "\n",
    "* **Cell State (Memory)**: Imagine this as a \"conveyor belt\" for information, running through the entire sequence. It carries information across time steps, allowing the network to retain relevant data for long periods.\n",
    "* **Gates**: Three specialized \"gates\" control the flow of information into and out of the cell state:\n",
    "    * **Forget Gate**: Decides what information to discard from the previous cell state.\n",
    "    * **Input Gate**: Determines how much of the new candidate information (derived from the current input and previous hidden state) should be added to the cell state.\n",
    "    * **Output Gate**: Controls how much of the current cell state will contribute to the hidden state, which then serves as the output for the current time step and input for the next.\n",
    "\n",
    "![](lstm-cell.png)\n",
    "\n",
    ": Figure 2: The internal structure of a Long Short-Term Memory (LSTM) cell.\n",
    "\n",
    "Figure 2 visually represents these gates and how they interact to selectively update and output information, making LSTMs adept at **remembering crucial details over long sequences**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffd264",
   "metadata": {},
   "source": [
    "# The Limitations of Individual Networks\n",
    "\n",
    "While **CNNs and LSTMs** are incredibly powerful in their respective domains, they have significant **limitations** when dealing with data that exhibits both strong spatial and temporal dependencies.\n",
    "\n",
    "## CNNs: Good at Space but Bad at Time\n",
    "\n",
    "Standard CNNs, while **excellent at extracting features** from individual images or frames but they **do not inherently capture temporal dependencies between consecutive frames.** When you feed a sequence of images (like a video) into a standard CNN, it treats each frame as an independent input. This means it can recognize objects within each frame, but it loses crucial information about motion, changes, or the sequence of events over time. For example, a CNN could identify a car in two consecutive frames, but it wouldn't inherently understand that the car is moving or how it's moving.\n",
    "\n",
    "## LSTMs: Good at Time but Bad at Space\n",
    "\n",
    "Traditional LSTMs are **excellent for sequences**, but they **expect a 1D vector** as input at each time step. This presents a major challenge for spatial data like images or video frames. To feed an image (e.g., a 64x64 pixel grayscale image) into a standard LSTM, you would first have to **\"flatten\" (unroll)** its 2D grid into a long 1D vector (e.g., 4096 pixels).\n",
    "\n",
    "This \"flattening\" process leads to two significant problems:\n",
    "\n",
    "1. **Loss of Spatial Relationships**: When you flatten an image, you destroy the crucial spatial relationships between pixels. Pixels that were close together in the 2D grid become distant in the 1D vector. The LSTM then loses the ability to recognize patterns based on proximity, adjacency, or overall shape â€“ the very strengths of CNNs.\n",
    "2. **High Dimensionality**: For higher-resolution images or colored images, flattening can lead to extremely long input vectors, making the LSTM computationally expensive, prone to overfitting, and harder to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c9658",
   "metadata": {},
   "source": [
    "# The Solution: ConvLSTM\n",
    "\n",
    "Recognizing the limitations of relying solely on CNNs for temporal tasks or LSTMs for spatial tasks, ConvLSTM emerged as a powerful hybrid architecture. It integrates:\n",
    "\n",
    "* **Convolutional Neural Networks (CNNs)**: To expertly extract spatial features from grid-like data (like images).\n",
    "* **Long Short-Term Memory (LSTMs)**: To learn and remember long-term dependencies in sequential data.\n",
    "\n",
    "**ConvLSTM** was specifically developed to **process data that has both spatial and temporal dimensions.** simultaneously. This makes it ideal for sequential data where each data point is a multi-dimensional grid, such as:\n",
    "\n",
    "* Video frames\n",
    "* Weather radar data\n",
    "* Medical imaging sequences (e.g., fMRI, dynamic MRI)\n",
    "\n",
    "Crucially, ConvLSTM **maintains the spatial information** throughout its recurrent connections, unlike traditional LSTMs that would flatten the input, thus providing a much more **effective way to model spatio-temporal dynamics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2b716",
   "metadata": {},
   "source": [
    "# How ConvLSTM Works\n",
    "\n",
    "The core innovation of ConvLSTM lies in how it adapts the internal operations of an LSTM cell to handle spatial data directly.\n",
    "\n",
    "## The 5D Input\n",
    "\n",
    "Before diving into the mechanics, let's understand the typical input data shape for a ConvLSTM layer. It's usually a 5D tensor with the following dimensions:\n",
    "\n",
    "`(batch_size, timesteps, height, width, channels)`\n",
    "\n",
    "Let's break down each dimension:\n",
    "\n",
    "* `batch_size`: The number of independent sequences processed simultaneously (for parallel computation).\n",
    "* `timesteps`: The length of the sequence (e.g., the number of video frames in a clip).\n",
    "* `height`: The spatial height of each input frame/grid.\n",
    "* `width`: The spatial width of each input frame/grid.\n",
    "* `channels`: The number of feature channels for each input frame/grid (e.g., 3 for an RGB image, 1 for grayscale).\n",
    "\n",
    "This 5D structure is crucial because it allows the ConvLSTM to operate on the full spatial dimensions of your data at each time step.\n",
    "\n",
    "## Convolutions Inside the LSTM loop\n",
    "\n",
    "In a traditional LSTM, the gates (Forget, Input, Output) and the candidate cell state generation perform matrix multiplications with their inputs (current input and previous hidden state).\n",
    "\n",
    "In ConvLSTM, all these **matrix multiplications are fundamentally replaced by convolutional operations**. This means that ,the Forget Gate, the Input Gate, the Output Gate and the candidate cell state generation all utilises 2D or 3D, depending on your data and implementation convolutional filters. This is how CNNs work inside the LSTM loop.\n",
    "\n",
    "![](convlstm.png)\n",
    "\n",
    ": Figure 3: A conceptual diagram of a ConvLSTM cell, highlighting how all internal matrix multiplications are replaced by convolutional operations (denoted by the orange asterisk)\n",
    "\n",
    "As shown in Figure 3, the inputs $X_t$, $H_{t-1}$, and $C_{t-1}$ are no longer flattened vectors. Instead, they are spatial feature maps (or the raw image/frame), and the weights ($W_{XC}$, $W_{XH}$, etc.) are now convolutional kernals. These kernels slide across the spatial dimensions of the input, preserving the spatial hierarchy.\n",
    "\n",
    "## Preserving Spatial Hierarchy and Learning Temporal Dependencies\n",
    "\n",
    "By replacing matrix multiplications with convolutions, ConvLSTM achieves two critical advantages:\n",
    "\n",
    "1. **Spatial Feature Learning**: Just like a CNN, it can learn and extract spatial features (edges, textures, object parts) from each input frame or spatial slice at every time step.  \n",
    "2. **Temporal Dependency Modeling**: Like an LSTM, it can maintain and update an internal cell state across time, allowing it to learn long-term dependencies and patterns in the sequence.\n",
    "\n",
    "This means that instead of just learning what to remember or forget (as in a regular LSTM), a ConvLSTM learns what spatial patterns to remember or forget over time, and how those patterns evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167693c8",
   "metadata": {},
   "source": [
    "# Simple Implementation on ConvLSTM Next Frame Prediction\n",
    "\n",
    "Before we dive into the ConvLSTM implementation, let's take a look at the dataset. This section uses a short video clip, randomly sourced online. While its exact subject matter is unknown (my best guess involves some form of gaseous movement!), it provides an excellent and visually engaging sequence for our ConvLSTM model.\n",
    "\n",
    "{{< video dataset.mp4 >}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "src_path = r'./dataset'\n",
    "\n",
    "length = len(os.listdir(src_path))\n",
    "\n",
    "images = []\n",
    "for i in range(length):\n",
    "    img = Image.open(f'{src_path}/img{i}.png')\n",
    "    img=img.crop((0,120,640,360))\n",
    "    img=img.convert('RGB')\n",
    "    img=img.resize((160,60),Image.Resampling.LANCZOS)\n",
    "    img=np.array(img)/255\n",
    "    img=img.reshape(60,160,3)\n",
    "    images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(images,step_size,start,stop):\n",
    "    seqx=[]\n",
    "    seqy=[]\n",
    "    x=[]\n",
    "    y=[]\n",
    "\n",
    "    sample_size=stop-start\n",
    "\n",
    "    for i in range(start,stop):\n",
    "        endx=i+step_size\n",
    "        endy=i+step_size*2\n",
    "\n",
    "        seqx.append(images[i:endx])\n",
    "        seqy.append(images[endx:endy])\n",
    "\n",
    "        x.append(np.array(seqx))\n",
    "        y.append(np.array(seqy))\n",
    "\n",
    "        seqx.clear()\n",
    "        seqy.clear()\n",
    "\n",
    "    return np.array(x).reshape(sample_size,step_size, 60, 160, 3),np.array(y).reshape(sample_size,step_size,60, 160, 3)\n",
    "\n",
    "step_size = 3\n",
    "\n",
    "xtrain,ytrain = train_test(images,step_size,0,800)\n",
    "xtest,ytest = train_test(images,step_size,900,990)\n",
    "xval,yval = train_test(images,step_size,800,900)\n",
    "print(f'train: x{xtrain.shape},y{ytrain.shape}  test: x{xtest.shape},y:{ytest.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99655d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D,BatchNormalization,Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def model():\n",
    "    seq = Sequential()\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       input_shape=(step_size, 60, 160, 3),\n",
    "                       padding='same', return_sequences=True,activation=\"relu\"))\n",
    "    seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True,activation=\"relu\"))\n",
    "    seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True,activation=\"relu\"))\n",
    "    seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                       padding='same', return_sequences=True,activation=\"relu\"))\n",
    "    seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(Conv3D(filters=3, kernel_size=(3, 3, 3),\n",
    "                   activation='sigmoid',\n",
    "                   padding='same', data_format='channels_last'))\n",
    "    seq.compile(loss='mae', optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "    return seq\n",
    "\n",
    "m=model()\n",
    "m.summary()\n",
    "\n",
    "m.fit(xtrain,ytrain,epochs=15,batch_size=5,\n",
    "validation_data=(xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model,xtest,ytest):\n",
    "    choice = np.random.choice(len(xtest))\n",
    "    print(choice)\n",
    "    pre = model.predict(xtest[choice].reshape(1,step_size, 60, 160, 3))\n",
    "    pre=pre*255\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 4))\n",
    "\n",
    "    for time, ax in enumerate(axes[0]):\n",
    "        ax.imshow(np.squeeze(ytest[choice][time]))\n",
    "\n",
    "        ax.set_title(f\"Ground Truth {time+1}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for time, ax in enumerate(axes[1]):\n",
    "        im=Image.fromarray(np.uint8(pre[0][time]),'RGB')\n",
    "        ax.imshow(im)\n",
    "        ax.set_title(f\"Prediction {time+1}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "predictions(m,xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1885655",
   "metadata": {},
   "source": [
    "# Real-World Applications of ConvLSTM\n",
    "\n",
    "The ability of ConvLSTM to concurrently model spatial and temporal dynamics makes it suitable for a wide array of real-world applications:\n",
    "\n",
    "* **Video Prediction / Next Frame Prediction**: This is a direct application, as shown in our example. ConvLSTM can learn the motion and evolution of objects and patterns within a video, generating realistic future frames. This has implications for:\n",
    "    * **Traffic Flow Prediction**: Anticipating vehicle movement on roads.  \n",
    "    * **Sports Analysis**: Predicting ball trajectories or player movements.  \n",
    "* **Weather Forecasting and Climate Modeling**: Predicting spatio-temporal patterns of meteorological phenomena like rainfall, temperature maps, or storm movements over time. Radar data, which is essentially a sequence of spatial maps, is a perfect fit.  \n",
    "* **Action Recognition in Videos**: Identifying human actions or activities (e.g., walking, running, clapping) by analyzing the sequence of spatial features extracted from video frames.  \n",
    "* **Medical Image Analysis (Time Series)**: Analyzing sequences of medical scans (e.g., fMRI for brain activity, dynamic MRI for organ movement, ultrasound videos) to detect changes, track disease progression, or diagnose conditions. Examples include:\n",
    "    * **Tumor Growth Monitoring**: Tracking changes in tumor size and shape over time.\n",
    "    * **Cardiac Motion Analysis**: Assessing heart wall movement from cine MRI sequences.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
